# -*- coding: utf-8 -*-
"""Stad1 of  ADSYN-ENN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-mp_iLbEw5W_rkVVZKT130Ir0MYPnKhn

#IDA
"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from ipywidgets import interact_manual
import plotly.express as px

from google.colab import drive

drive.mount('/content/drive')

# Replace 'your_file_path' with the actual path to your CSV file in Google Drive
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/stroke.csv')

df.columns
df.info()

df.drop('id',axis=1,inplace=True)

df.shape

df.describe()

#Checking NA
df.isnull().sum()

"""#Data Cleaning"""

# impute bmi column with mean and print  first ten

df['bmi'] = df['bmi'].fillna(df['bmi'].mean())
df.head(10)

df.isnull().sum()

numerical = df.select_dtypes(include=['int64','float64']).columns.tolist()
numerical

df.hypertension.value_counts()

plt.figure(figsize=(10,6))
plt.pie(df.hypertension.value_counts(),labels=['No Hypertension','Hypertension'],autopct='%1.1f%%',
        colors=sns.color_palette('mako',2),explode=(0.3,0.1),shadow=True,textprops={'fontsize':12,'style':'italic'})

df.heart_disease.value_counts()

plt.figure(figsize=(10,6))
plt.pie(df.heart_disease.value_counts(),labels=['No Heart disease','Heart disease'],autopct='%1.1f%%',
        colors=sns.color_palette('mako',2),explode=(0.3,0.1),shadow=True,textprops={'fontsize':12,'style':'italic'})

plt.figure(figsize=(10,6))
sns.histplot(df.avg_glucose_level,kde=True,bins=30)

plt.figure(figsize=(10,6))
sns.histplot(df.bmi,kde=True,bins=30)

"""#Outliers"""

plt.figure(figsize=(10,6))
sns.boxplot(df.age)

plt.figure(figsize=(10,6))
sns.boxplot(df.avg_glucose_level)

plt.figure(figsize=(10,6))
sns.boxplot(df.bmi)

# prompt: handle outlier from avg_glucose_level and bmi dont remove it

import numpy as np
def handle_outliers_with_capping(df, column):
  """Handles outliers by capping them at a specified percentile."""
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR

  df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
  df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])

  return df

df = handle_outliers_with_capping(df, 'avg_glucose_level')
df = handle_outliers_with_capping(df, 'bmi')

# prompt: print boxplot back

import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
sns.boxplot(df.avg_glucose_level)
plt.figure(figsize=(10,6))
sns.boxplot(df.bmi)

"""# Scalling numeric data"""

# prompt: standardize only age, avg_glucose_level and bmi

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Select the columns to standardize
cols_to_standardize = ['age', 'avg_glucose_level', 'bmi']

# Fit and transform the selected columns
df[cols_to_standardize] = scaler.fit_transform(df[cols_to_standardize])

df.head()

df.describe()

"""# Scalling discrete data"""

categories = df.select_dtypes(include=['object']).columns.tolist()
categories

df.gender.value_counts()

df.drop(df[df.gender == 'Other'].index,inplace=True)
df.reset_index(drop=True,inplace=True)

plt.figure(figsize=(10,6))
sns.countplot(x="gender", data=df, palette='mako',hue='stroke')
plt.legend(['no stroke','stroke'])

df.gender = df.gender.map({'Male':0,'Female':1})

df.ever_married.value_counts()

plt.figure(figsize=(10,6))
sns.countplot(x="ever_married", data=df, palette='mako',hue='stroke')
plt.legend(['no stroke','stroke'])

df.ever_married = df.ever_married.map({'No':0,'Yes':1})

df.work_type.value_counts()

plt.figure(figsize=(10,6))
sns.countplot(x="work_type", data=df, palette='mako',hue='stroke')
plt.legend(['no stroke','stroke'])

# prompt: for work_type i want one hot encoding with value 1 and 0 under each column 1 for false and 0 for true

import pandas as pd
# Create dummy variables for 'work_type'
work_type_dummies = pd.get_dummies(df['work_type'], prefix='work_type', dtype=int)

# Concatenate the dummy variables with the original DataFrame
df = pd.concat([df, work_type_dummies], axis=1)

# Drop the original 'work_type' column
df.drop('work_type', axis=1, inplace=True)

# Display the updated DataFrame
df.head()

df.Residence_type.value_counts()

plt.figure(figsize=(10,6))
sns.countplot(x="Residence_type", data=df, palette='mako',hue='stroke')
plt.legend(['no stroke','stroke'])

df.Residence_type = df.Residence_type.map({'Urban':0,'Rural':1})

df.smoking_status.value_counts()

plt.figure(figsize=(10,6))
sns.countplot(x="smoking_status", data=df, palette='mako',hue='stroke')
plt.legend(['no stroke','stroke'])

# prompt: for smoking_status i want one hot encoding with value 1 and 0 under each column 1 for false and 0 for true

import pandas as pd
# Create dummy variables for 'smoking_status'
smoking_status_dummies = pd.get_dummies(df['smoking_status'], prefix='smoking_status', dtype=int)

# Concatenate the dummy variables with the original DataFrame
df = pd.concat([df, smoking_status_dummies], axis=1)

# Drop the original 'smoking_status' column
df.drop('smoking_status', axis=1, inplace=True)

# Display the updated DataFrame
df.head()

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(),annot=True)

# prompt: check for skewness

# Check skewness
for feature in df:
  print(f"Skewness of {feature}: {df[feature].skew()}")

# prompt: check missing value

df.isnull().sum()

x = df.drop('stroke', axis = 1)
y = pd.to_numeric( df['stroke'])

from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x,y,test_size = .20)

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import ADASYN
from imblearn.under_sampling import EditedNearestNeighbours  # Import ENN

# Create an ADASYN object (for over-sampling)
adasyn = ADASYN()

# Create an Edited Nearest Neighbors object (for under-sampling)
enn = EditedNearestNeighbours()

# Create a pipeline combining ADASYN and ENN
adasyn_enn = Pipeline([('adasyn', adasyn), ('enn', enn)])  # Change TomekLinks to ENN

# Apply ADASYN + ENN to the training data
X_resampled, y_resampled = adasyn_enn.fit_resample(x_train, y_train)  # Ensure variable names are correct, x_train is lowercase

# prompt: show result before and after sampling

import pandas as pd
# Before Sampling
print("Before Sampling:")
print(y_train.value_counts())

# After Sampling
print("\nAfter Sampling (ADASYN + Tomek Links):")
print(pd.Series(y_resampled).value_counts())

"""#Random Forest"""

!pip install scikit-learn
from sklearn.metrics import classification_report # import the missing module
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier # import the missing module
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix # import the missing function
import pandas as pd

X_train,X_test,y_train,y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,random_state=42)
model = RandomForestClassifier(random_state=1234)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test,y_pred))
ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred),display_labels=['No Stroke','Stroke']).plot()

# prompt: plot ROC AUC

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have y_test and y_pred from your model
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""#Logistic regression"""

!pip install scikit-learn
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression  # Import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
import pandas as pd

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Instantiate the Logistic Regression model
model = LogisticRegression(random_state=1234, max_iter=1000)  # Increased max_iter for convergence
model.fit(X_train, y_train)  # Fit the model

# Make predictions
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# Display the confusion matrix
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=['No Stroke', 'Stroke']).plot()

# prompt: plot roc

import matplotlib.pyplot as plt
# Assuming you have y_test and y_pred_proba from your model
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""# Naive bayes

"""

!pip install scikit-learn
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB  # Import GaussianNB for Naive Bayes
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
import pandas as pd

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Instantiate the Naive Bayes model
model = GaussianNB()  # Create an instance of GaussianNB
model.fit(X_train, y_train)  # Fit the model

# Make predictions
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# Display the confusion matrix
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=['No Stroke', 'Stroke']).plot()

# prompt: plot ROC

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Assuming you have y_test and y_pred_proba from your model
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# prompt: perform ANN 5 layer

import matplotlib.pyplot as plt
!pip install tensorflow

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define the model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Test accuracy:', accuracy)

# Make predictions
y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(int)

# Print the classification report
print(classification_report(y_test, y_pred))

# Display the confusion matrix
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=['No Stroke', 'Stroke']).plot()

# Assuming you have y_test and y_pred_proba from your model
y_pred_proba = model.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# prompt: perform XGBoost

import matplotlib.pyplot as plt
!pip install xgboost

from xgboost import XGBClassifier

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Instantiate the XGBoost model
model = XGBClassifier(random_state=1234)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# Display the confusion matrix
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=['No Stroke', 'Stroke']).plot()


# Assuming you have y_test and y_pred_proba from your model
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()